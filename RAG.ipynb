{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DOWNLOADING AND IMPORTIN DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_ollama\n",
      "  Downloading langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_ollama) (0.3.39)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain_ollama)\n",
      "  Using cached ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\mushaf mughal\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ollama<1,>=0.4.4->langchain_ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mushaf mughal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.3.1)\n",
      "Downloading langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
      "Using cached ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain_ollama\n",
      "Successfully installed langchain_ollama-0.2.3 ollama-0.4.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_community\n",
    "# !pip install pypdf\n",
    "# !pip install fastembed\n",
    "# !pip install chromadb\n",
    "# !pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from groq import Groq\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **INITIALIZING VECTOR DATABASE OF OUR SOURCE PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 30 documents into 48 chunks.\n"
     ]
    }
   ],
   "source": [
    "def ingest():\n",
    "    # Get the doc\n",
    "    loader = PyPDFLoader(\"User Manual.pdf\")\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    # Split the pages by char\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\"],\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    # for i in range(len(chunks)):\n",
    "    #     print(f\"Chunk {i}: {chunks[i].page_content}\")\n",
    "\n",
    "\n",
    "    # Create vector store\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=\"./chroma_vector_db\")\n",
    "\n",
    "ingest() #run only first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **INITIALIZING LLMS INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_llama(prompt):\n",
    "\n",
    "  client = Groq(api_key=\"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\")\n",
    "\n",
    "  system_prompt = \"\"\"You are given a question and a context. Answer the question **only if** the context clearly and directly supports it.\n",
    "  \n",
    "  **INSTRUCTIONS:**\n",
    "  - Do NOT make assumptions, guesses, or inferred answers.\n",
    "  - If the context does NOT explicitly contain the information needed to answer, respond with:\n",
    "    `No context available for this question.`\n",
    "\n",
    "  - If the question is a greeting (e.g., \"hi\", \"hello\", \"hey\") or a meta-question (e.g., \"what can you do?\", \"who are you?\", \"tell me about yourself\"), respond with:\n",
    "    `I'm a RAG bot from PromptPay, created to help you with FAQ queries related to the PromptPay banking app. Ask me anything about it!`  \n",
    "\n",
    "  - Otherwise, if the context is missing, unrelated, or insufficient, always respond:\n",
    "    `No context available for this question.`\n",
    "\n",
    "  - Your response must ONLY contain the final answer string. No explanation, formatting, or extra text.\n",
    "\n",
    "  **PERFORM NOW ON THESE INPUTS:**\n",
    "  \"\"\"\n",
    "  \n",
    "  chat_completion = client.chat.completions.create(\n",
    "      model=\"llama-3.3-70b-versatile\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": str(system_prompt)},\n",
    "          {\"role\": \"user\", \"content\": str(prompt)},\n",
    "      ],\n",
    "  )\n",
    "  return chat_completion.choices[0].message.content\n",
    "\n",
    "def call_llm_gemini(prompt):\n",
    "\n",
    "    client = genai.Client(api_key=\"AIzaSyARwVVMeMDar13QXZ6y_W2ZIZik0VU-cF8\")\n",
    "\n",
    "    system_prompt = \"\"\"You are given a question and a context. Answer the question **only if** the context clearly and directly supports it.\n",
    "    \n",
    "    **INSTRUCTIONS:**\n",
    "    - Do NOT make assumptions, guesses, or inferred answers.\n",
    "    - If the context does NOT explicitly contain the information needed to answer, respond with:\n",
    "      `No context available for this question.`\n",
    "\n",
    "    - If the question is a greeting (e.g., \"hi\", \"hello\", \"hey\") or a meta-question (e.g., \"what can you do?\", \"who are you?\", \"tell me about yourself\"), respond with:\n",
    "      `I'm a RAG bot from PromptPay, created to help you with FAQ queries related to the PromptPay banking app. Ask me anything about it!`  \n",
    "\n",
    "    - Otherwise, if the context is missing, unrelated, or insufficient, always respond:\n",
    "      `No context available for this question.`\n",
    "\n",
    "    - Your response must ONLY contain the final answer string. No explanation, formatting, or extra text.\n",
    "\n",
    "    **PERFORM NOW ON THESE INPUTS:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_prompt),\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **INITIALIZING RAG CHAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain():\n",
    "\n",
    "    # Hey there! I'm your PromptPay assistant, here to help you with questions about the PromptPay banking app. Ask me anything you need!\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Question: {input}\n",
    "    Context: {context}\n",
    "    \"\"\")\n",
    "    \n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    vector_store = Chroma(persist_directory=\"./chroma_vector_db\", embedding_function=embedding)\n",
    "\n",
    "    #Create chain\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"score_threshold\": 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(call_llm_gemini, prompt)\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    return chain\n",
    "\n",
    "def ask(query: str):\n",
    "    chain = rag_chain()\n",
    "    result = chain.invoke({\"input\": query})\n",
    "    print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **INFERENCING THE RAG APPLICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a RAG bot from PromptPay, created to help you with FAQ queries related to the PromptPay banking app. Ask me anything about it!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"hello how are you?\")  # Greeting Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer will login to NBP Digital \n",
      "• NBP Digital home screen will be displayed \n",
      "• Customer access to hamburger menu from home screen \n",
      "• Customer taps on the change password option \n",
      "• Following input fields will appear on change password screen: \n",
      "i. Old Password \n",
      "ii. New Password \n",
      "iii. Re-enter Password \n",
      "• Customer provide the required information & press submit button \n",
      "• Once done, confirmation notification will appear “Dear Customer, Your Password has been changed successfully!”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"how to change password?\")  # Relevant Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No context available for this question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"how can i book usa flight?\")  # Irrelevant Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
